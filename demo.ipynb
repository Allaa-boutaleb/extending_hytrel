{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import search.unionable_table_search as uts\n",
    "import search.joinable_table_search as jts\n",
    "import post_processing.filtering_reranking as fr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](pipeline_illustration.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unionable Table Search \n",
    "Once the data lake embeddings have been computed following the instructions described in [here](embedding_computation/README.md), we can execute unionable table search. <br>\n",
    "In this demo, we have already computed embeddings for SANTOS data lake provided with SANTOS benchmark with the following configurations [#0](embedding_computation/experiments.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalake_embeddings = './output/hytrel_embedding/santos/hytrel_datalake_columns_0.pkl'\n",
    "query_embeddings = './output/hytrel_embedding/santos/hytrel_query_columns_0.pkl'\n",
    "k = 10\n",
    "## select a query dataset\n",
    "with open(query_embeddings, 'rb') as f:\n",
    "    query_columns_hytrel = pickle.load(f)\n",
    "with open(datalake_embeddings, 'rb') as f:\n",
    "    datalake_columns_hytrel = pickle.load(f)\n",
    "query_table = query_columns_hytrel[0] ## corresponding to 'cihr_co-applicant_b.csv'\n",
    "candidates, build_duration, query_duration = uts.approximate_unionable_dataset_search([query_columns_hytrel[0]], datalake_columns_hytrel,k,compress_method='max')\n",
    "candidates = candidates['cihr_co-applicant_b.csv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate in 1 place: cihr_co-applicant_b.csv\n",
      "candidate in 2 place: cihr_co-applicant_9.csv\n",
      "candidate in 3 place: cihr_co-applicant_7.csv\n",
      "candidate in 4 place: cihr_co-applicant_6.csv\n",
      "candidate in 5 place: cihr_co-applicant_8.csv\n",
      "candidate in 6 place: cihr_co-applicant_3.csv\n",
      "candidate in 7 place: cihr_co-applicant_5.csv\n",
      "candidate in 8 place: cihr_co-applicant_1.csv\n",
      "candidate in 9 place: cihr_co-applicant_0.csv\n",
      "candidate in 10 place: cihr_co-applicant_2.csv\n",
      "Index build duration: 0.0013661384582519531\n",
      "Query duration: 8.20159912109375e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candidates)):\n",
    "    print(f'candidate in {i+1} place: {candidates[i]}')\n",
    "\n",
    "print(f'Index build duration: {build_duration}')\n",
    "print(f'Query duration: {query_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-based search \n",
    "Once the embeddings have been computed. We can preform hierarchal clustering on the computed data lake embeddings. Instructions to compute heirarchal clustering can be found [here](clustering/) <br>\n",
    "In this demo, we have ran clustering on column embeddings with configuration [0](/embedding_computation/experiments.md) and cluster count of [811](clustering/experiments.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m clustering \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/clustering/santos/clustering_811_santos_run_id_0.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(clustering, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     clustering_result \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m datalake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(clustering_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()))\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcihr_co-applicant_b.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/hytrel-emb/lib/python3.9/site-packages/pandas/core/internals/blocks.py:2728\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim, refs)\u001b[0m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_block\u001b[39m(\n\u001b[1;32m   2717\u001b[0m     values,\n\u001b[1;32m   2718\u001b[0m     placement: BlockPlacement,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2725\u001b[0m     \u001b[38;5;66;03m# - check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[1;32m   2726\u001b[0m     \u001b[38;5;66;03m# - maybe_coerce_values already called/unnecessary\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m     klass \u001b[38;5;241m=\u001b[39m get_block_type(values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)"
     ]
    }
   ],
   "source": [
    "clustering = './output/clustering/santos/clustering_811_santos_run_id_0.pkl'\n",
    "with open(clustering, 'rb') as f:\n",
    "    clustering_result = pickle.load(f)\n",
    "datalake = list(set(clustering_result['dataset'].unique()))\n",
    "query = 'cihr_co-applicant_b.csv'\n",
    "k = 10\n",
    "res = uts.unionable_table_search_using_clustering([query], datalake, clustering_result,k)\n",
    "candidates = res[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate in 1 place: cihr_co-applicant_b.csv\n",
      "candidate in 2 place: cihr_co-applicant_9.csv\n",
      "candidate in 3 place: cihr_co-applicant_7.csv\n",
      "candidate in 4 place: cihr_co-applicant_6.csv\n",
      "candidate in 5 place: cihr_co-applicant_8.csv\n",
      "candidate in 6 place: cihr_co-applicant_3.csv\n",
      "candidate in 7 place: cihr_co-applicant_5.csv\n",
      "candidate in 8 place: cihr_co-applicant_1.csv\n",
      "candidate in 9 place: cihr_co-applicant_0.csv\n",
      "candidate in 10 place: cihr_co-applicant_2.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candidates)):\n",
    "    print(f'candidate in {i+1} place: {candidates[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joinable Table Search\n",
    "Once the data lake embeddings have been computed following the instructions described in [here](embedding_computation/README.md), we can execute unionable table search. <br>\n",
    "In this demo, we have already computed embeddings for NextiaJD testbedS data lake provided with NextiaJD benchmark with the following configurations [#4](embedding_computation/experiments.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_datalake_columns:  2553\n",
      "build regular index using faiss\n",
      "number of queries: 1\n"
     ]
    }
   ],
   "source": [
    "datalake_embeddings = './output/hytrel_embedding/nextiajd/testbedS/hytrel_datalake_columns_4.pkl'\n",
    "query_embeddings = './output/hytrel_embedding/nextiajd/testbedS/hytrel_query_columns_4.pkl'\n",
    "with open(query_embeddings, 'rb') as f:\n",
    "    query_columns_hytrel = pickle.load(f)\n",
    "with open(datalake_embeddings, 'rb') as f:\n",
    "    datalake_columns_hytrel = pickle.load(f)\n",
    "\n",
    "res, build_duration, query_duration = jts.joinable_dataset_search([query_columns_hytrel[61]], datalake_columns_hytrel,1000,'testbedS')\n",
    "candidates = res[('agri-environmental-indicators-emissions-by-sector.csv',\n",
    "  'Area')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate in 1 place: ('emissions_agriculture_energy_e_all_data_norm.csv', 'Country')\n",
      "candidate in 2 place: ('global-greenhouse-gas-emissions0.csv', 'Country')\n",
      "candidate in 3 place: ('global-innovation-index-2015.csv', 'COUNTRY_NAME')\n",
      "candidate in 4 place: ('population-estimates-and-projections-1960-2050.csv', 'COUNTRY')\n",
      "candidate in 5 place: ('global-greenhouse-gas-emissions.csv', 'Country')\n",
      "candidate in 6 place: ('listings_summary.csv', 'host_name')\n",
      "candidate in 7 place: ('makemytrip_com-travel_sample.csv', 'city')\n",
      "candidate in 8 place: ('listings_detailed.csv', 'host_name')\n",
      "candidate in 9 place: ('listings_summary.csv', 'host_neighbourhood')\n",
      "candidate in 10 place: ('listings_summary.csv', 'neighbourhood')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candidates)):\n",
    "    print(f'candidate in {i+1} place: {candidates[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing: Filtering, and Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LSH ensemble for query ('agri-environmental-indicators-emissions-by-sector.csv', 'Area')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alaaalmutawa/Documents/Thesis/nextiajd/testbedS/datasets/agri-environmental-indicators-emissions-by-sector.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datalake_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/alaaalmutawa/Documents/Thesis/nextiajd/testbedS/datasets\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m filtered,overlap_est \u001b[38;5;241m=\u001b[39m \u001b[43mfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_lsh_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalake_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_part\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PHD/HyTrel/extending_hytrel/post_processing/filtering_reranking.py:97\u001b[0m, in \u001b[0;36mrun_lsh_ensemble\u001b[0;34m(datalake_dir, candidate_tables, num_perm, threshold, num_part)\u001b[0m\n\u001b[1;32m     95\u001b[0m query_table \u001b[38;5;241m=\u001b[39m query[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     96\u001b[0m query_column \u001b[38;5;241m=\u001b[39m query[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m res,est\u001b[38;5;241m=\u001b[39m \u001b[43mlsh_ensemble_all_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalake_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_tables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_perm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_part\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_part\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m all_res[query] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     99\u001b[0m all_est[query] \u001b[38;5;241m=\u001b[39m est\n",
      "File \u001b[0;32m~/Desktop/PHD/HyTrel/extending_hytrel/post_processing/filtering_reranking.py:44\u001b[0m, in \u001b[0;36mlsh_ensemble_all_tables\u001b[0;34m(datalake_dir, candidate_tables, query_table, query_column, num_perm, threshold, num_part)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlsh_ensemble_all_tables\u001b[39m(datalake_dir, candidate_tables, query_table, query_column,num_perm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, num_part\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     q_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalake_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquery_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     set1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(q_df[query_column]) \u001b[38;5;66;03m## minhash for the query table\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     m1 \u001b[38;5;241m=\u001b[39m MinHash(num_perm\u001b[38;5;241m=\u001b[39mnum_perm)\n",
      "File \u001b[0;32m~/Desktop/PHD/HyTrel/extending_hytrel/post_processing/filtering_reranking.py:19\u001b[0m, in \u001b[0;36mget_df\u001b[0;34m(file_path, delimiter)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_df\u001b[39m(file_path,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 19\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m \u001b[43mdetermine_delimiter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path:\u001b[39m\u001b[38;5;124m'\u001b[39m,file_path)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# if 'sewer-manholes.csv' in file_path or 'water-distribution-mains.csv' in file_path or 'district-wise-rainfall-data-for-india-2014.csv' in file_path or 'water-control-valves.csv' in file_path\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PHD/HyTrel/extending_hytrel/post_processing/filtering_reranking.py:8\u001b[0m, in \u001b[0;36mdetermine_delimiter\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetermine_delimiter\u001b[39m(filepath):\n\u001b[1;32m      7\u001b[0m      \u001b[38;5;66;03m# Determine delimiter based on file content\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m             first_line \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m first_line:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/alaaalmutawa/Documents/Thesis/nextiajd/testbedS/datasets/agri-environmental-indicators-emissions-by-sector.csv'"
     ]
    }
   ],
   "source": [
    "datalake_source = '/Users/alaaalmutawa/Documents/Thesis/nextiajd/testbedS/datasets'\n",
    "filtered,overlap_est = fr.run_lsh_ensemble(datalake_source, res, num_perm=256, threshold=0.5, num_part=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('agri-environmental-indicators-emissions-by-sector.csv',\n",
       "  'Area'): [('emissions_agriculture_energy_e_all_data_norm.csv',\n",
       "   'Country'), ('global-greenhouse-gas-emissions0.csv', 'Country'), ('global-greenhouse-gas-emissions.csv',\n",
       "   'Country'), ('global-innovation-index-2015.csv', 'COUNTRY_NAME')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate in 1 place: ('emissions_agriculture_energy_e_all_data_norm.csv', 'Country') in the original list ('emissions_agriculture_energy_e_all_data_norm.csv', 'Country')\n",
      "candidate in 2 place: ('global-greenhouse-gas-emissions0.csv', 'Country') in the original list ('global-greenhouse-gas-emissions0.csv', 'Country')\n",
      "candidate in 3 place: ('global-greenhouse-gas-emissions.csv', 'Country') in the original list ('global-innovation-index-2015.csv', 'COUNTRY_NAME')\n",
      "candidate in 4 place: ('global-innovation-index-2015.csv', 'COUNTRY_NAME') in the original list ('population-estimates-and-projections-1960-2050.csv', 'COUNTRY')\n",
      "candidate in 5 place: ('population-estimates-and-projections-1960-2050.csv', 'COUNTRY') in the original list ('global-greenhouse-gas-emissions.csv', 'Country')\n",
      "candidate in 6 place: ('listings_summary.csv', 'host_name') in the original list ('listings_summary.csv', 'host_name')\n",
      "candidate in 7 place: ('makemytrip_com-travel_sample.csv', 'city') in the original list ('makemytrip_com-travel_sample.csv', 'city')\n",
      "candidate in 8 place: ('listings_detailed.csv', 'host_name') in the original list ('listings_detailed.csv', 'host_name')\n",
      "candidate in 9 place: ('listings_summary.csv', 'host_neighbourhood') in the original list ('listings_summary.csv', 'host_neighbourhood')\n",
      "candidate in 10 place: ('listings_summary.csv', 'neighbourhood') in the original list ('listings_summary.csv', 'neighbourhood')\n"
     ]
    }
   ],
   "source": [
    "reranked = fr.rank_table(overlap_est)\n",
    "candidates_reranked = reranked[('agri-environmental-indicators-emissions-by-sector.csv',\n",
    "  'Area')]\n",
    "for i in range(len(candidates_reranked)):\n",
    "    print(f'candidate in {i+1} place: {candidates_reranked[i]} in the original list {candidates[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hytrel-emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
